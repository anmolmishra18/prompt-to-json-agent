# ğŸ¤– Prompt-to-JSON Agent

A reinforcement learning agent that converts free-text or structured prompts into **refined JSON specifications** through an iterative evaluation and feedback loop.

## ğŸš€ Features

- **Prompt to JSON Conversion**: Accepts structured input (`Title: ...; Description: ...; Owner: ...; Requirements: ...`) or free text
- **Iterative Refinement**: Uses evaluation + feedback loop to improve specs over multiple iterations
- **Comprehensive Reporting**: Generates JSON and TXT reports for each iteration
- **Feedback Logging**: Maintains detailed logs of all improvements and iterations
- **Dual Interface**: Command Line Interface (CLI) and Streamlit web app
- **Values Tracking**: Daily logs for honesty, discipline, gratitude, and integrity

## ğŸ›  Installation

```bash
git clone https://github.com/your-username/prompt-to-json-agent.git
cd prompt-to-json-agent
python -m venv venv
source venv/bin/activate   # On Linux/Mac
venv\\Scripts\\activate      # On Windows
pip install -r requirements.txt
```

## ğŸ’» Usage

### 1. CLI Mode

**Run with structured prompt:**
```bash
python main.py --prompt "Title: Student Portal; Description: Manage courses and grades; Owner: Anmol; Requirements: login, dashboard, reports" --iterations 3
```

**Run with free-text prompt:**
```bash
python main.py --prompt "Create a student management system for tracking grades and attendance" --iterations 3
```

**Run with sample JSON file:**
```bash
python main.py --sample samples/spec1.json --iterations 2
```

### 2. Streamlit Web App

Launch the web interface:
```bash
streamlit run app.py
```

Enter your prompt and choose the number of feedback iterations. The app will show:
- The parsed JSON spec
- The final refined spec after iterations
- Links to generated reports and logs

## ğŸ“‚ Project Structure

```
prompt-to-json-agent/
â”‚
â”œâ”€â”€ main.py              # CLI entrypoint + pipeline orchestration
â”œâ”€â”€ app.py               # Streamlit web interface
â”œâ”€â”€ evaluator/
â”‚   â”œâ”€â”€ criteria.py      # JSON schema validation & scoring
â”‚   â”œâ”€â”€ feedback.py      # Feedback generation & application
â”‚   â””â”€â”€ report.py        # Report generation (JSON + TXT)
â”‚
â”œâ”€â”€ samples/             # Sample JSON specifications
â”‚   â”œâ”€â”€ spec1.json
â”‚   â””â”€â”€ spec2.json
â”‚
â”œâ”€â”€ reports/             # Generated reports + daily_log.txt
â”œâ”€â”€ logs/                # Feedback history logs
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md           # This documentation
```

## ğŸ”„ How It Works

1. **Input Processing**: Parses structured or free-text prompts into initial JSON spec
2. **Evaluation**: Scores the spec (0-100) based on completeness and quality
3. **Feedback Generation**: Identifies missing or inadequate fields
4. **Spec Refinement**: Applies suggested improvements
5. **Iteration**: Repeats steps 2-4 for specified number of iterations
6. **Reporting**: Generates comprehensive reports and logs

## ğŸ“Š Evaluation Criteria

The agent evaluates specs based on:
- **Schema Validation**: Adherence to required JSON structure
- **Completeness**: Presence of title, description, owner, priority, requirements
- **Quality**: Description length, requirement count, priority-based adjustments
- **Priority Handling**: Stricter evaluation for high-priority specs

## ğŸ§­ Daily Values Tracking

Each CLI run appends a reflection entry to `/reports/daily_log.txt`:

```
=== 20250828_112335 ===
Integrity: Submitted work honestly without skipping steps
Honesty: Tested full pipeline and verified all components work
Discipline: Maintained clean code structure and documentation
Gratitude: Grateful for completing all Task 3 requirements
```

## ğŸ¯ Task 3 Completion Status

âœ… **Day 1**: Repo setup + folder structure + base pipeline  
âœ… **Day 2**: Evaluator + report generator implemented  
âœ… **Day 3**: Feedback loop + RL agent working  
âœ… **Day 4**: CLI + Streamlit app + documentation complete  

## ğŸ¤ Contributing

This project follows the core values of:
- **Honesty**: Transparent reporting of what works vs. what's blocked
- **Discipline**: Regular commits and structured development
- **Gratitude**: Acknowledging contributions and learning
- **Integrity**: Honest evaluation and authentic progress tracking

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ‘¥ Acknowledgments

- Thanks to Olivia and Saad for evaluation logic contributions
- Grateful for open-source libraries: FastAPI, Streamlit, jsonschema
- AWS Q Developer for development assistance